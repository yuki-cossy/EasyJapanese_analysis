{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3bWEPen-TxR6","executionInfo":{"status":"ok","timestamp":1671687019677,"user_tz":-540,"elapsed":18543,"user":{"displayName":"Yuki “Cossy” Komo","userId":"17779653510058054287"}},"outputId":"079d8ded-721f-4f9f-c03f-896f52939925"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6FDsXNjuTKNZ","executionInfo":{"status":"ok","timestamp":1671686975215,"user_tz":-540,"elapsed":25,"user":{"displayName":"Yuki “Cossy” Komo","userId":"17779653510058054287"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","source":["# データ読み込み"],"metadata":{"id":"Ar_wyYF8cP9x"}},{"cell_type":"code","source":["T23_train = pd.read_csv('/content/drive/MyDrive/Learning/EasyJapanese/analysis/corpus/T23_train.csv')\n","T23_test = pd.read_csv('/content/drive/MyDrive/Learning/EasyJapanese/analysis/corpus/T23_test.csv')\n","T15 = pd.read_csv('/content/drive/MyDrive/Learning/EasyJapanese/analysis/corpus/T15.csv')"],"metadata":{"id":"L0nqoij2cSJR","executionInfo":{"status":"ok","timestamp":1671689386144,"user_tz":-540,"elapsed":718,"user":{"displayName":"Yuki “Cossy” Komo","userId":"17779653510058054287"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# データ前処理"],"metadata":{"id":"w_eZXrZiWWMF"}},{"cell_type":"code","source":["# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n","from __future__ import unicode_literals\n","import re\n","import unicodedata\n"," \n","def unicode_normalize(cls, s):\n","    pt = re.compile('([{}]+)'.format(cls))\n"," \n","    def norm(c):\n","        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n"," \n","    s = ''.join(norm(x) for x in re.split(pt, s))\n","    s = re.sub('－', '-', s)\n","    return s\n"," \n","def remove_extra_spaces(s):\n","    s = re.sub('[ ]+', ' ', s)\n","    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n","                      '\\u3040-\\u309F',  # HIRAGANA\n","                      '\\u30A0-\\u30FF',  # KATAKANA\n","                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n","                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n","                      ))\n","    basic_latin = '\\u0000-\\u007F'\n"," \n","    def remove_space_between(cls1, cls2, s):\n","        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n","        while p.search(s):\n","            s = p.sub(r'\\1\\2', s)\n","        return s\n"," \n","    s = remove_space_between(blocks, blocks, s)\n","    s = remove_space_between(blocks, basic_latin, s)\n","    s = remove_space_between(basic_latin, blocks, s)\n","    return s\n"," \n","def normalize_neologd(s):\n","    s = s.strip()\n","    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n"," \n","    def maketrans(f, t):\n","        return {ord(x): ord(y) for x, y in zip(f, t)}\n"," \n","    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n","    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n","    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n","    s = s.translate(\n","        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n","              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n"," \n","    s = remove_extra_spaces(s)\n","    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n","    s = re.sub('[’]', '\\'', s)\n","    s = re.sub('[”]', '\"', s)\n","    return s"],"metadata":{"id":"FHdW4GglV6Eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import numpy as np\n","import pickle\n","from tqdm import tqdm\n"," \n","tag_regex = re.compile(r\"<[^>]*?>\")\n"," \n","def normalize_text(text):\n","    text = text.replace(\"\\t\", \" \")\n","    text = normalize_neologd(text)\n","    text = tag_regex.sub(\"\", text)\n","    text = text.replace(\"&quot;\", \"\\\"\").replace(\"&amp;\", \"&\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&nbsp;\", \" \")\n","    return text\n"," \n","all_data = []\n","count = 0\n"," \n","for index, data in T23_train.iterrows():\n","    if data['#日本語(原文)'] is None or data['#日本語(原文)'] is np.nan or not data['#日本語(原文)']:\n","        continue\n","    if data['#やさしい日本語'] is None or data['#やさしい日本語'] is np.nan or not data['#やさしい日本語']:\n","        continue\n","    normalized_body = normalize_text(data['body'])\n","    all_data.append({\"text\": \"keyword: \" + normalized_body,\"response\": normalize_text(data['keyword_str_1']),})\n","    all_data.append({\"text\": \"keyword: \" + normalized_body,\"response\": normalize_text(data['keyword_str_2']),})\n","    all_data.append({\"text\": \"keyword: \" + normalized_body,\"response\": normalize_text(data['keyword_str_3']),})\n","    all_data.append({\"text\": \"topics_title: \" + normalized_body,\"response\": normalize_text(data['topics_article_title']),})\n","    all_data.append({\"text\": \"title: \" + normalized_body,\"response\": normalize_text(data['title']),})\n","    if data['long_title'] is not None and data['long_title'] is not np.nan:\n","        all_data.append({\"text\": \"long_title: \" + normalized_body,\"response\": normalize_text(data['long_title']),})\n","    if data['summary_1'] is not None and data['summary_1'] is not np.nan:\n","        all_data.append({\"text\": \"summary_1: \" + normalized_body,\"response\": normalize_text(data['summary_1']),})\n","    if data['summary_2'] is not None and data['summary_2'] is not np.nan:\n","        all_data.append({\"text\": \"summary_2: \" + normalized_body,\"response\": normalize_text(data['summary_2']),})\n","    if data['summary_3'] is not None and data['summary_3'] is not np.nan:\n","        all_data.append({\"text\": \"summary_3: \" + normalized_body,\"response\": normalize_text(data['summary_3']),})\n"],"metadata":{"id":"-aNZWDZHZCMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["T23_train.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StAmyy43dKWC","executionInfo":{"status":"ok","timestamp":1671689463839,"user_tz":-540,"elapsed":4,"user":{"displayName":"Yuki “Cossy” Komo","userId":"17779653510058054287"}},"outputId":"db5ee4ca-d22e-4a09-e463-db882800ab27"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 34300 entries, 0 to 34299\n","Data columns (total 5 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   ID        34300 non-null  object\n"," 1   #日本語(原文)  34300 non-null  object\n"," 2   #やさしい日本語  34300 non-null  object\n"," 3   #英語(原文)   34300 non-null  object\n"," 4   #固有名詞     2812 non-null   object\n","dtypes: object(5)\n","memory usage: 1.3+ MB\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2Q66jxuJdMGH"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"898d3c6109607cac4667983f45c21ef7997d125c9b4da8292caa90a38e599c34"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}